{
	"name": "event_hubs_streaming_clean",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "cdcsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b18146fc-8cd0-4066-ba5d-66a2b0ab473e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/91b6e3ed-6c42-4e7a-8ed3-57f49a2a42ba/resourceGroups/SB_RelDB_CDC_Streaming/providers/Microsoft.Synapse/workspaces/asa-cdc-streaming/bigDataPools/cdcsparkpool",
				"name": "cdcsparkpool",
				"type": "Spark",
				"endpoint": "https://asa-cdc-streaming.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/cdcsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import delta\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, FloatType, BooleanType, TimestampType, DecimalType"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"source": [
					"type_of_data = 'stock'\r\n",
					"\r\n",
					"keyVaultName = \"cdc-streaming-kv2\"\r\n",
					"secretName = \"eh-connstring\"\r\n",
					"connectionString = mssparkutils.credentials.getSecret(keyVaultName, secretName)\r\n",
					"EH_name = f\"{type_of_data}-00\"\r\n",
					"connectionStringEntity = connectionString + \";EntityPath=\" + EH_name\r\n",
					"consumer_group = \"$Default\"\r\n",
					"object_schema = type_of_data\r\n",
					"\r\n",
					"checkpoint_loc = f\"abfss://dlcdcstreaming@dlcdcstreaming.dfs.core.windows.net/landing/{type_of_data}/checkpoint/\"\r\n",
					"landing_loc = f\"abfss://dlcdcstreaming@dlcdcstreaming.dfs.core.windows.net/landing/{type_of_data}/output/\"\r\n",
					"staging_loc = f\"abfss://dlcdcstreaming@dlcdcstreaming.dfs.core.windows.net/staging/{type_of_data}/\"\r\n",
					"\r\n",
					"# Create the starting position Dictionary\r\n",
					"startingEventPosition = {\r\n",
					"  \"offset\": \"-1\",         \r\n",
					"  \"seqNo\": 0,                   # not in use\r\n",
					"  \"enqueuedTime\": None,         # not in use\r\n",
					"  \"isInclusive\": True,\r\n",
					"  \"fromStartOfStream\": True     # start from beginning of Stream\r\n",
					"}\r\n",
					"\r\n",
					"ehConf = {\r\n",
					"  \"eventhubs.startingPosition\" : json.dumps(startingEventPosition),\r\n",
					"  \"setMaxEventsPerTrigger\" : 100,\r\n",
					"  \"eventhubs.consumerGroup\" : consumer_group\r\n",
					"}\r\n",
					"\r\n",
					"ehConf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionStringEntity)"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#to be moved to schemabuild module\r\n",
					"\r\n",
					"OBJ_SCHEMA_MAP = {\r\n",
					"    'orders': (\r\n",
					"        StructType()    \\\r\n",
					"        .add(\"OrderID\", IntegerType()) \\\r\n",
					"        .add(\"CustomerID\", IntegerType())  \\\r\n",
					"        .add(\"SalespersonPersonID\", IntegerType()) \\\r\n",
					"        .add(\"PickedByPersonID\", IntegerType())    \\\r\n",
					"        .add(\"ContactPersonID\", IntegerType()) \\\r\n",
					"        .add(\"BackorderOrderID\", IntegerType())    \\\r\n",
					"        .add(\"OrderDate\", IntegerType())   \\\r\n",
					"        .add(\"ExpectedDeliveryDate\", IntegerType())    \\\r\n",
					"        .add(\"CustomerPurchaseOrderNumber\", StringType())  \\\r\n",
					"        .add(\"IsUndersupplyBackordered\", BooleanType())    \\\r\n",
					"        .add(\"Comments\", StringType()) \\\r\n",
					"        .add(\"DeliveryInstructions\", StringType())   \\\r\n",
					"        .add(\"InternalComments\", StringType()) \\\r\n",
					"        .add(\"PickingCompletedWhen\", LongType())   \\\r\n",
					"        .add(\"LastEditedBy\", IntegerType())    \\\r\n",
					"        .add(\"LastEditedWhen\", LongType())\r\n",
					"        ),\r\n",
					"    'stock': (\r\n",
					"        StructType()    \\\r\n",
					"        .add(\"StockItemID\", IntegerType()) \\\r\n",
					"        .add(\"StockItemName\", StringType()) \\\r\n",
					"        .add(\"SupplierID\", IntegerType())  \\\r\n",
					"        .add(\"ColorID\", IntegerType()) \\\r\n",
					"        .add(\"UnitPackageID\", IntegerType())    \\\r\n",
					"        .add(\"OuterPackageID\", IntegerType()) \\\r\n",
					"        .add(\"Brand\", StringType()) \\\r\n",
					"        .add(\"Size\", StringType()) \\\r\n",
					"        .add(\"LeadTimeDays\", IntegerType())    \\\r\n",
					"        .add(\"QuantityPerOuter\", IntegerType())   \\\r\n",
					"        .add(\"IsChillerStock\", BooleanType())    \\\r\n",
					"        .add(\"Barcode\", StringType())  \\\r\n",
					"        .add(\"TaxRate\", DecimalType())    \\\r\n",
					"        .add(\"UnitPrice\", DecimalType())   \\\r\n",
					"        .add(\"RecommendedRetailPrice\", DecimalType()) \\\r\n",
					"        .add(\"TypicalWeightPerUnit\", DecimalType())   \\\r\n",
					"        .add(\"MarketingComments\", StringType())    \\\r\n",
					"        .add(\"InternalComments\", StringType())    \\\r\n",
					"        .add(\"Photo\", StringType())    \\\r\n",
					"        .add(\"CustomFields\", StringType()) \\\r\n",
					"        .add(\"Tags\", StringType()) \\\r\n",
					"        .add(\"SearchDetails\", StringType()) \\\r\n",
					"        .add(\"LastEditedBy\", IntegerType())    \\\r\n",
					"        .add(\"ValidFrom\", LongType())    \\\r\n",
					"        .add(\"ValidTo\", LongType())\r\n",
					"    ),\r\n",
					"}\r\n",
					"    \r\n",
					"def get_cdc_schema(obj_name: str):\r\n",
					"    # if obj_name not in OBJ_SCHEMA_MAP.keys():\r\n",
					"    #     raise KeyError\r\n",
					"    obj_schema = OBJ_SCHEMA_MAP[obj_name]\r\n",
					"    return (\r\n",
					"        StructType()\r\n",
					"        .add(\"op\", StringType())\r\n",
					"        .add(\"ts_ms\", LongType())\r\n",
					"        .add(\"before\", obj_schema)\r\n",
					"        .add(\"after\", obj_schema)\r\n",
					"    )\r\n",
					"\r\n",
					"def deserialize_cdc_msg(df: DataFrame, cdc_schema: StructType, root_schema: StructType):\r\n",
					"        df_parsed = df.withColumn(\"parsed\", col(\"body\").cast(\"string\"))\r\n",
					"        df_payload = df_parsed  \\\r\n",
					"                        .select(from_json(col(\"parsed\"), root_schema)   \\\r\n",
					"                        .alias(\"parsed_json\")).select(\"parsed_json.payload\")\r\n",
					"        return (\r\n",
					"            df_payload.withColumn('value', from_json(to_json('payload'), cdc_schema))    \\\r\n",
					"                .withColumn('result', when(col('value.op') == \"d\", col('value.before'))   \\\r\n",
					"                    .otherwise(col('value.after')))\r\n",
					"            .select(    \\\r\n",
					"            'value.op', \\\r\n",
					"            'result.*',    \\\r\n",
					"            (col('value.ts_ms') / 1000).cast(TimestampType()).alias('ts'))\r\n",
					"        )\r\n",
					"       \r\n",
					"def deserialize_cdc_msg_json(df: DataFrame, cdc_schema: StructType):\r\n",
					"        df = df.select('payload')\r\n",
					"        return (\r\n",
					"            df.withColumn('value', from_json(to_json('payload'), cdc_schema))    \\\r\n",
					"                .withColumn('result', when(col('value.op') == \"d\", col('value.before'))   \\\r\n",
					"                    .otherwise(col('value.after')))\r\n",
					"                .select(    \\\r\n",
					"                'value.op', \\\r\n",
					"                'result.*',    \\\r\n",
					"                (col('value.ts_ms') / 1000).cast(TimestampType()).alias('ts'))\r\n",
					"        )"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"INNER_SCHEMA_MAP = {\r\n",
					"    'orders': [\r\n",
					"        StructField(\"OrderID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"CustomerID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"SalespersonPersonID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"PickedByPersonID\", IntegerType(), nullable=True),\r\n",
					"        StructField(\"ContactPersonID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"BackorderOrderID\", IntegerType(), nullable=True),\r\n",
					"        StructField(\"OrderDate\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"ExpectedDeliveryDate\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"CustomerPurchaseOrderNumber\", StringType(), nullable=True),\r\n",
					"        StructField(\"IsUndersupplyBackordered\", BooleanType(), nullable=False),\r\n",
					"        StructField(\"Comments\", StringType(), nullable=True),\r\n",
					"        StructField(\"DeliveryInstructions\", StringType(), nullable=True),\r\n",
					"        StructField(\"InternalComments\", StringType(), nullable=True),\r\n",
					"        StructField(\"PickingCompletedWhen\", LongType(), nullable=True),\r\n",
					"        StructField(\"LastEditedBy\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"LastEditedWhen\", LongType(), nullable=False)\r\n",
					"    ],\r\n",
					"    'stock': [\r\n",
					"        StructField(\"StockItemID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"StockItemName\", StringType(), nullable=False),\r\n",
					"        StructField(\"SupplierID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"ColorID\", IntegerType(), nullable=True),\r\n",
					"        StructField(\"UnitPackageID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"OuterPackageID\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"Brand\", StringType(), nullable=True),\r\n",
					"        StructField(\"Size\", StringType(), nullable=True),\r\n",
					"        StructField(\"LeadTimeDays\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"QuantityPerOuter\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"IsChillerStock\", BooleanType(), nullable=False),\r\n",
					"        StructField(\"Barcode\", StringType(), nullable=True),\r\n",
					"        StructField(\"TaxRate\", DecimalType(), nullable=False),\r\n",
					"        StructField(\"UnitPrice\", DecimalType(), nullable=False),\r\n",
					"        StructField(\"RecommendedRetailPrice\", DecimalType(), nullable=True),\r\n",
					"        StructField(\"TypicalWeightPerUnit\", DecimalType(), nullable=False),\r\n",
					"        StructField(\"MarketingComments\", StringType(), nullable=True),\r\n",
					"        StructField(\"InternalComments\", StringType(), nullable=True),\r\n",
					"        StructField(\"Photo\", StringType(), nullable=True),\r\n",
					"        StructField(\"CustomFields\", StringType(), nullable=True),\r\n",
					"        StructField(\"Tags\", StringType(), nullable=True),\r\n",
					"        StructField(\"SearchDetails\", StringType(), nullable=False),\r\n",
					"        StructField(\"LastEditedBy\", IntegerType(), nullable=False),\r\n",
					"        StructField(\"ValidFrom\", LongType(), nullable=False),\r\n",
					"        StructField(\"ValidTo\", LongType(), nullable=False)\r\n",
					"    ],\r\n",
					"}\r\n",
					"    "
				],
				"execution_count": 69
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_raw = spark \\\r\n",
					"  .readStream \\\r\n",
					"  .format(\"eventhubs\") \\\r\n",
					"  .options(**ehConf) \\\r\n",
					"  .load()\r\n",
					"\r\n",
					"# df_parsed = df_raw.withColumn(\"parsed\", col(\"body\").cast(\"string\"))"
				],
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Check if is streaming\r\n",
					"# print(df_raw.isStreaming)\r\n",
					"# print(\"-----\")\r\n",
					"# df_raw.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cdc_schema = get_cdc_schema(object_schema)\r\n",
					"\r\n",
					"# if inner_struct_fields:\r\n",
					"#     inner_struct_fields.clear()\r\n",
					"\r\n",
					"inner_struct_fields = INNER_SCHEMA_MAP[object_schema]\r\n",
					"inner_struct_fields"
				],
				"execution_count": 71
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the outer struct fields\r\n",
					"outer_struct_fields = [\r\n",
					"    StructField(\"before\", StructType(inner_struct_fields), nullable=True),\r\n",
					"    StructField(\"after\", StructType(inner_struct_fields), nullable=True),\r\n",
					"    StructField(\"source\", StructType([\r\n",
					"        StructField(\"version\", StringType(), nullable=False),\r\n",
					"        StructField(\"connector\", StringType(), nullable=False),\r\n",
					"        StructField(\"name\", StringType(), nullable=False),\r\n",
					"        StructField(\"ts_ms\", LongType(), nullable=False),\r\n",
					"        StructField(\"snapshot\", StringType(), nullable=True),\r\n",
					"        StructField(\"db\", StringType(), nullable=False),\r\n",
					"        StructField(\"sequence\", StringType(), nullable=True),\r\n",
					"        StructField(\"schema\", StringType(), nullable=False),\r\n",
					"        StructField(\"table\", StringType(), nullable=False),\r\n",
					"        StructField(\"change_lsn\", StringType(), nullable=True),\r\n",
					"        StructField(\"commit_lsn\", StringType(), nullable=True),\r\n",
					"        StructField(\"event_serial_no\", LongType(), nullable=True)\r\n",
					"    ]), nullable=False),\r\n",
					"    StructField(\"op\", StringType(), nullable=False),\r\n",
					"    StructField(\"ts_ms\", LongType(), nullable=True),\r\n",
					"    StructField(\"transaction\", StructType([\r\n",
					"        StructField(\"id\", StringType(), nullable=False),\r\n",
					"        StructField(\"total_order\", LongType(), nullable=False),\r\n",
					"        StructField(\"data_collection_order\", LongType(), nullable=False)\r\n",
					"    ]), nullable=True)\r\n",
					"]\r\n",
					"\r\n",
					"# Define the root schema\r\n",
					"root_schema = StructType([\r\n",
					"    StructField(\"schema\", StructType([\r\n",
					"        StructField(\"type\", StringType(), nullable=False),\r\n",
					"        StructField(\"fields\", StructType(outer_struct_fields), nullable=False)\r\n",
					"    ]), nullable=False),\r\n",
					"    StructField(\"payload\", StructType(outer_struct_fields), nullable=True)\r\n",
					"])"
				],
				"execution_count": 72
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_deserialized = deserialize_cdc_msg(df_raw, cdc_schema, root_schema)\r\n",
					"df_deserialized.printSchema()"
				],
				"execution_count": 73
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# query_test = df_deserialized  \\\r\n",
					"# \t\t\t\t.writeStream    \\\r\n",
					"# \t\t\t\t.format(\"json\") \\\r\n",
					"# \t\t\t\t.outputMode(\"append\")   \\\r\n",
					"# \t\t\t\t.option(\"checkpointLocation\", checkpoint_loc)   \\\r\n",
					"# \t\t\t\t.option(\"path\", landing_loc)    \\\r\n",
					"# \t\t\t\t.start()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# query_test.stop()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Check the status of query\r\n",
					"\r\n",
					"# print(query_test.isActive)\r\n",
					"# print(\"-----\")\r\n",
					"# print(query_test.exception())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"CDCquery.stop()"
				],
				"execution_count": 102
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Streaming Query in scheduler pool\r\n",
					"spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool0\")\r\n",
					"\r\n",
					"CDCquery = df_deserialized \\\r\n",
					"    .withColumn(\"year\", year(\"ts\"))   \\\r\n",
					"    .withColumn(\"month\", month(\"ts\"))   \\\r\n",
					"    .withColumn(\"day\", dayofmonth(\"ts\"))   \\\r\n",
					"    .withColumn(\"hour\", hour(\"ts\"))   \\\r\n",
					"    .writeStream    \\\r\n",
					"    .queryName(\"CDCquery\") \\\r\n",
					"    .format(\"delta\")  \\\r\n",
					"    .partitionBy(\"year\", \"month\", \"day\", \"hour\")    \\\r\n",
					"    .outputMode(\"append\")   \\\r\n",
					"    .option(\"checkpointLocation\", checkpoint_loc)   \\\r\n",
					"    .option(\"path\", landing_loc)    \\\r\n",
					"    .start()"
				],
				"execution_count": 103
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check the status of query\r\n",
					"\r\n",
					"print(CDCquery.isActive)\r\n",
					"print(\"-----\")\r\n",
					"print(CDCquery.exception())"
				],
				"execution_count": 104
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_latest_records(df, id_col, ts_col):\r\n",
					"    latest_records = df.groupBy(id_col).agg(max(ts_col).alias(f\"max_{ts_col}\"))\r\n",
					"    result = df.join(latest_records, (df.StockItemID == latest_records.StockItemID) & (df.ts == latest_records.max_ts), \"inner\")\r\n",
					"    result = result.select(df['*'])\r\n",
					"    return result\r\n",
					"\r\n",
					"def add_checksum(df):\r\n",
					"    # Create a checksum of all fields without timestamp and opeartion type\r\n",
					"    cols = df.columns\r\n",
					"    non_checksum = ['ts','op']\r\n",
					"    for item in non_checksum:\r\n",
					"        cols.remove(item)\r\n",
					"    df_checksum = df.withColumn(\"checksum\", md5(concat_ws(\"\", *cols)))\r\n",
					"    return df_checksum"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"raw_path = f\"{landing_loc}year=*/month=*/day=*/hour=*/*.parquet\"\r\n",
					"df_init = spark.read.format(\"parquet\").load(raw_path)\r\n",
					"\r\n",
					"df_init = get_latest_records(df_init,id_col='StockItemID',ts_col='ts')\r\n",
					"df_init = add_checksum(df_init)\r\n",
					"\r\n",
					"# df_init.show()\r\n",
					"df_init.write.mode(\"append\").saveAsTable(\"StockItems_init\")\r\n",
					"# df_init.createOrReplaceTempView(\"StockItems_init\")"
				],
				"execution_count": 105
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Initial load to staging\r\n",
					"df_staging = df_init \\\r\n",
					"            .withColumn(\"CurrentFlag\", lit(1))   \\\r\n",
					"            .withColumn(\"EffDateFrom\", lit(current_timestamp())) \\\r\n",
					"            .withColumn(\"EffDateTo\", lit(None).cast(TimestampType()))   \\\r\n",
					"            .drop(\"op\", \"ValidFrom\", \"ValidTo\")\r\n",
					"\r\n",
					"df_staging.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(staging_loc)\r\n",
					""
				],
				"execution_count": 106
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Initial staging table creation\r\n",
					"\r\n",
					"-- DROP TABLE IF EXISTS StockItems_SCD2\r\n",
					"\r\n",
					"CREATE TABLE StockItems_SCD2 USING delta\r\n",
					"LOCATION 'abfss://dlcdcstreaming@dlcdcstreaming.dfs.core.windows.net/staging/stock/'"
				],
				"execution_count": 109
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql \r\n",
					"DROP TABLE IF EXISTS StockItems_updates"
				],
				"execution_count": 110
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"raw_path = f\"{landing_loc}year=*/month=*/day=*/hour=*/*.parquet\"\r\n",
					"# Before running delete all files in landing directory before merging new updates not to cause overwriting in SCD2 table during merge\r\n",
					"\r\n",
					"df_updates = spark.read.format(\"parquet\").load(raw_path)\r\n",
					"\r\n",
					"# Obtain only latest state for specific id and latest timestamp\r\n",
					"df_updates = get_latest_records(df_updates, id_col='StockItemID', ts_col='ts')\r\n",
					"\r\n",
					"# Create a checksum of all fields without timestamp and opeartion type\r\n",
					"df_updates = add_checksum(df_updates)\r\n",
					"\r\n",
					"df_updates.write.mode(\"append\").saveAsTable(\"StockItems_updates\")"
				],
				"execution_count": 113
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- -- RESTORE TO ORIGINAL VERSION BEFORE REFLECTING UPDATES\r\n",
					"-- RESTORE TABLE StockItems_SCD2 TO VERSION AS OF 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- First step:\r\n",
					"-- Insert updated records by comparing checksum. The obsolete records will be marked as CurrentFlag=0 in the next step\r\n",
					"-- If there is a delete row, it is intserted \r\n",
					"\r\n",
					"INSERT INTO StockItems_SCD2\r\n",
					"    -- (StockItemID,StockItemName,SupplierID,ColorID,UnitPackageID,OuterPackageID,Brand,Size,LeadTimeDays,QuantityPerOuter,IsChillerStock,Barcode,TaxRate,UnitPrice,\r\n",
					"    -- RecommendedRetailPrice,TypicalWeightPerUnit,MarketingComments,InternalComments,Photo,CustomFields,Tags,SearchDetails,LastEditedBy,CurrentFlag,EffDateFrom,EffDateTo)\r\n",
					"SELECT src.StockItemID,src.StockItemName,src.SupplierID,src.ColorID,src.UnitPackageID,src.OuterPackageID,src.Brand,src.Size,src.LeadTimeDays,src.QuantityPerOuter,src.IsChillerStock,src.Barcode,src.TaxRate,src.UnitPrice,\r\n",
					"    src.RecommendedRetailPrice,src.TypicalWeightPerUnit,src.MarketingComments,src.InternalComments,src.Photo,src.CustomFields,src.Tags,src.SearchDetails,src.LastEditedBy,src.ts,1,CURRENT_TIMESTAMP(),\r\n",
					"    CASE WHEN (src.op='d') then CURRENT_TIMESTAMP() ELSE make_timestamp(9999, 12, 31, 00, 00, 00) END,src.checksum\r\n",
					"FROM StockItems_updates src\r\n",
					"    JOIN StockItems_SCD2 tgt \r\n",
					"    ON src.StockItemID = tgt.StockItemID\r\n",
					"    WHERE (src.checksum <> tgt.checksum) AND tgt.CurrentFlag=1;\r\n",
					"\r\n",
					"-- Second step:\r\n",
					"-- Merge other updates with SCD2 table\r\n",
					"\r\n",
					"MERGE INTO StockItems_SCD2 AS tgt\r\n",
					"USING\r\n",
					"(\r\n",
					"    SELECT\r\n",
					"    StockItemID,StockItemName,SupplierID,ColorID,UnitPackageID,OuterPackageID,Brand,Size,LeadTimeDays,QuantityPerOuter,IsChillerStock,Barcode,TaxRate,UnitPrice,\r\n",
					"    RecommendedRetailPrice,TypicalWeightPerUnit,MarketingComments,InternalComments,Photo,CustomFields,Tags,SearchDetails,LastEditedBy,checksum\r\n",
					"    FROM StockItems_updates\r\n",
					"    ) AS src \r\n",
					"ON (src.StockItemID = tgt.StockItemID AND tgt.CurrentFlag = 1)\r\n",
					"-- Deactivate flag and set EffectiveDateTo for obsolete records, replaced for the ones inserted in previous step\r\n",
					"WHEN MATCHED and tgt.checksum <> src.checksum and tgt.CurrentFlag=1 THEN\r\n",
					"    UPDATE\r\n",
					"    SET tgt.CurrentFlag = 0, tgt.EffDateTo = CURRENT_TIMESTAMP()\r\n",
					"-- Insert brand new records with unmatched IDs\r\n",
					"WHEN NOT MATCHED THEN\r\n",
					"    INSERT \r\n",
					"    (StockItemID,StockItemName,SupplierID,ColorID,UnitPackageID,OuterPackageID,Brand,Size,LeadTimeDays,QuantityPerOuter,IsChillerStock,Barcode,TaxRate,UnitPrice,\r\n",
					"    RecommendedRetailPrice,TypicalWeightPerUnit,MarketingComments,InternalComments,Photo,CustomFields,Tags,SearchDetails,LastEditedBy,checksum,CurrentFlag,EffDateFrom,EffDateTo,checksum)\r\n",
					"    VALUES \r\n",
					"    (StockItemID,StockItemName,SupplierID,ColorID,UnitPackageID,OuterPackageID,Brand,Size,LeadTimeDays,QuantityPerOuter,IsChillerStock,Barcode,TaxRate,UnitPrice,\r\n",
					"    RecommendedRetailPrice,TypicalWeightPerUnit,MarketingComments,InternalComments,Photo,CustomFields,Tags,SearchDetails,LastEditedBy,checksum,1,CURRENT_TIMESTAMP(),make_timestamp(9999, 12, 31, 00, 00, 00),checksum);"
				],
				"execution_count": 129
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"select currentflag, * from stockitems_scd2\r\n",
					"WHERE StockItemID in \r\n",
					"    -- (select distinct stockitemid from stockitems_scd2 where CurrentFlag = 0)\r\n",
					"    (53, 999)\r\n",
					"order by stockitemid asc, currentflag desc"
				],
				"execution_count": 128
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql \r\n",
					"CREATE TABLE QuantityPerSupplier AS\r\n",
					"SELECT \r\n",
					"    SupplierID,\r\n",
					"    SUM(QuantityPerOuter) AS TotalQuantityPerOuter,\r\n",
					"    LAG(TotalPreviousQuantityPerOuter) OVER (PARTITION BY SupplierID ORDER BY EffDateTo) AS PreviousTotalQuantityPerOuter\r\n",
					"FROM \r\n",
					"(\r\n",
					"    SELECT \r\n",
					"        SupplierID,\r\n",
					"        EffDateTo,\r\n",
					"        CASE WHEN CurrentFlag = 1 THEN QuantityPerOuter ELSE 0 END AS QuantityPerOuter,\r\n",
					"        SUM(CASE WHEN CurrentFlag = 0 THEN QuantityPerOuter ELSE 0 END) OVER (PARTITION BY SupplierID ORDER BY EffDateTo) AS TotalPreviousQuantityPerOuter\r\n",
					"    FROM \r\n",
					"        StockItems_SCD2\r\n",
					") subquery\r\n",
					"WHERE \r\n",
					"    CurrentFlag = 1\r\n",
					"GROUP BY \r\n",
					"    SupplierID;\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}